# config for train.py

## ───────────────────────────────────── ▼ ─────────────────────────────────────
# {{{                           --     Init    --
#···············································································
[[init]]
wandb_project = 'Multilingual-Machine-Translation'
output_dir = 'fr-ru-outdir'
model_checkpoint = 'google/mt5-small'
dataset_name = 'wmt14'
lang_pairs = ['fr-ru']
max_seq_length = 64
preprocessing_num_workers = 32
split = 'None'
all_langs=false
#                                                                            }}}
## ─────────────────────────────────────────────────────────────────────────────



## ───────────────────────────────────── ▼ ─────────────────────────────────────
# {{{                             --    Training Args      --
#···············································································
[[train]]
batch_size = 2
accum_iter = 8
learning_rate = 5e-4
weight_decay = 0.00
dropout_rate = 0.1
num_train_epochs = 1
eval_every_steps = 20000
logging_steps = 10
max_train_steps = 0
lr_scheduler_type = 'linear'
num_warmup_steps = 5000
beam_size = 3
seed = 'None'
#                                                                            }}}
## ─────────────────────────────────────────────────────────────────────────────




